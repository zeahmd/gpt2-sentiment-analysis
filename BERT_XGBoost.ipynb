{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERT XGBoost.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "71b7cbff29974f2c99df56bdda992d2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ffd8b17ddb3e4833a723cfd8d5cdfef1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e909b455cf40422a8b9e2fcaa86c4281",
              "IPY_MODEL_d3bb65ba840248858a4f4fbc0f814a42"
            ]
          }
        },
        "ffd8b17ddb3e4833a723cfd8d5cdfef1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e909b455cf40422a8b9e2fcaa86c4281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5da3171966e5435eb2776fe5ce80ae35",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7584ebfe41cf4062b5498ce661dccfd2"
          }
        },
        "d3bb65ba840248858a4f4fbc0f814a42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d363ca9ad8f448fe9e457b9bd3ea533c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 1.54MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d5919865e97442899d4e79435b667f0c"
          }
        },
        "5da3171966e5435eb2776fe5ce80ae35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7584ebfe41cf4062b5498ce661dccfd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d363ca9ad8f448fe9e457b9bd3ea533c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d5919865e97442899d4e79435b667f0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EdHVQrqyDB50"
      },
      "source": [
        "# Memory Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E58HaeA6CsM9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "5f923304-38ea-4fe4-f9d5-53e165f283f2"
      },
      "source": [
        "import psutil\n",
        "def get_size(bytes, suffix=\"B\"):\n",
        "    factor = 1024\n",
        "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
        "        if bytes < factor:\n",
        "            return f\"{bytes:.2f}{unit}{suffix}\"\n",
        "        bytes /= factor\n",
        "print(\"=\"*40, \"Memory Information\", \"=\"*40)\n",
        "svmem = psutil.virtual_memory()\n",
        "print(f\"Total: {get_size(svmem.total)}\") ; print(f\"Available: {get_size(svmem.available)}\")\n",
        "print(f\"Used: {get_size(svmem.used)}\") ; print(f\"Percentage: {svmem.percent}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======================================== Memory Information ========================================\n",
            "Total: 25.51GB\n",
            "Available: 24.52GB\n",
            "Used: 651.97MB\n",
            "Percentage: 3.9%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FgVfBlvZDEja"
      },
      "source": [
        "# GPU Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rxlkxvkrCyin",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "edc3cd93-18aa-4d98-9c69-41f963caba08"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Sep  4 07:16:04 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0    35W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Csh8jY90_i0r",
        "colab_type": "text"
      },
      "source": [
        "# **Training BERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v6WR5Uaa6RQJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "3a697094-88fb-4627-ff1b-0a976f2785e2"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytreebank in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.2.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.6.0+cu101)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (3.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.41.1)\n",
            "Requirement already satisfied: loguru in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (7.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (0.22.2.post1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->-r requirements.txt (line 2)) (0.16.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers->-r requirements.txt (line 3)) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers->-r requirements.txt (line 3)) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers->-r requirements.txt (line 3)) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers->-r requirements.txt (line 3)) (0.8.1rc2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers->-r requirements.txt (line 3)) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers->-r requirements.txt (line 3)) (0.7)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers->-r requirements.txt (line 3)) (0.1.91)\n",
            "Requirement already satisfied: aiocontextvars>=0.2.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from loguru->-r requirements.txt (line 5)) (0.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->-r requirements.txt (line 7)) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (0.16.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (1.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers->-r requirements.txt (line 3)) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: contextvars==2.4; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiocontextvars>=0.2.0; python_version < \"3.7\"->loguru->-r requirements.txt (line 5)) (2.4)\n",
            "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars==2.4; python_version < \"3.7\"->aiocontextvars>=0.2.0; python_version < \"3.7\"->loguru->-r requirements.txt (line 5)) (0.14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jyDODoaWC6KI",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
        "import torch\n",
        "from dataset import SSTDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from utils import transformer_params\n",
        "from utils import evaluation_metrics, save_model, root_and_binary_title\n",
        "from math import ceil\n",
        "from loguru import logger\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from datetime import timedelta\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7dKuBUn6VLI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "89bd5e8a-fb2f-47b9-e73b-ec6bbac517f4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvb92NGS6dVm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7e403af6-e5cb-42f8-9f8b-3662568a8822"
      },
      "source": [
        "!ls /content/drive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Colab Notebooks'   Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YQFaW79Rdi0R",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C4_cQpe_c4BN",
        "colab": {}
      },
      "source": [
        "def load_transformer(name, binary):\n",
        "    config = BertConfig.from_pretrained('bert-base-uncased')\n",
        "    if not binary:\n",
        "      config.num_labels = 5\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    \n",
        "    return {'model': model,\n",
        "            'tokenizer': tokenizer}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FcSgyPvydj0s",
        "colab": {}
      },
      "source": [
        "def train_step(model, inputs, labels, optimizer):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss, logits = model(inputs['input_ids'], attention_mask=inputs['attention_mask'],\n",
        "                         labels=labels)[:2]\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return logits, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7984ypaMdixv",
        "colab": {}
      },
      "source": [
        "def eval_step(model, inputs, labels):\n",
        "    labels = labels.unsqueeze(0)\n",
        "    loss, logits = model(inputs['input_ids'], attention_mask=inputs['attention_mask']\n",
        "                         , labels=labels)[:2]\n",
        "\n",
        "    return logits, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tbHHlscCdivT",
        "colab": {}
      },
      "source": [
        "def train_epoch(model, tokenizer, train_dataset, optimizer, batch_size):\n",
        "    train_loader = DataLoader(dataset=train_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True)\n",
        "\n",
        "    correct_count = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    with tqdm(total=ceil(len(train_dataset)/batch_size), desc='train', unit='batch') as pbar:\n",
        "        for text, sentiment in train_loader:\n",
        "            text = tokenizer(text, padding=True, return_tensors='pt').to(device)\n",
        "            sentiment = sentiment.to(device)\n",
        "\n",
        "            logits, loss = train_step(model, text, sentiment, optimizer)\n",
        "\n",
        "            preds = torch.argmax(logits, axis=1)\n",
        "            correct_count += (preds == sentiment).sum().item()\n",
        "            total_loss += loss.item()\n",
        "            pbar.update(1)\n",
        "\n",
        "    return correct_count / len(train_dataset), total_loss / len(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xPxnxXajditk",
        "colab": {}
      },
      "source": [
        "def eval_epoch(model, tokenizer, eval_dataset, batch_size, split):\n",
        "    eval_loader = DataLoader(dataset=eval_dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=True)\n",
        "\n",
        "    correct_count = 0\n",
        "    total_loss = 0\n",
        "    y_pred = list()\n",
        "    y_true = list()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        with tqdm(total=ceil(len(eval_dataset)/batch_size), desc=split, unit='batch') as pbar:\n",
        "            for text, sentiment in eval_loader:\n",
        "                text = tokenizer(text, padding=True, return_tensors='pt').to(device)\n",
        "                sentiment = sentiment.to(device)\n",
        "\n",
        "                logits, loss = eval_step(model, text, sentiment)\n",
        "\n",
        "                preds = torch.argmax(logits, axis=1)\n",
        "                y_pred += preds.cpu().numpy().tolist()\n",
        "                y_true += sentiment.cpu().numpy().tolist()\n",
        "\n",
        "                correct_count += (preds == sentiment).sum().item()\n",
        "                total_loss += loss.item()\n",
        "                pbar.update(1)\n",
        "\n",
        "    metrics_score = evaluation_metrics(y_true, y_pred, split=split)\n",
        "    return correct_count / len(eval_dataset), total_loss / len(eval_dataset), metrics_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1JQeDe5FdirO",
        "colab": {}
      },
      "source": [
        "def train(name, root, binary, epochs=25, patience=3, save=False):\n",
        "\n",
        "    #load model and tokenizer..\n",
        "    try:\n",
        "        transformer_container = load_transformer(name, binary)\n",
        "    except ValueError:\n",
        "        logger.error(\"Invalid transformer name!\")\n",
        "        os._exit(0)\n",
        "    model = transformer_container['model']\n",
        "    model = model.to(device)\n",
        "    tokenizer = transformer_container['tokenizer']\n",
        "\n",
        "    #load batch_size and learning rate..\n",
        "    params_container = transformer_params(name)\n",
        "    batch_size = params_container['batch_size']\n",
        "    learning_rate = params_container['learning_rate']\n",
        "\n",
        "    #load train, dev and test datasets..\n",
        "    train_dataset = SSTDataset(root=root, binary=binary, split='train')\n",
        "    dev_dataset = SSTDataset(root=root, binary=binary, split='dev')\n",
        "    test_dataset = SSTDataset(root=root, binary=binary, split='test')\n",
        "\n",
        "    #Intialize optimizer..\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    #Initialize training variables..\n",
        "    best_acc = 0.0\n",
        "    best_loss = np.inf\n",
        "    stopping_step = 0\n",
        "    best_model_name = None\n",
        "\n",
        "    total_train_seconds = 0\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        start = time.time()\n",
        "        train_acc, train_loss = train_epoch(model, tokenizer, train_dataset, optimizer, batch_size)\n",
        "        end = time.time()\n",
        "        total_train_seconds += (end - start)\n",
        "        logger.info(f\"epoch: {epoch+1}, transformer: {name}, train_loss: {train_loss:.4f}, train_acc: {train_acc*100:.2f}\")\n",
        "\n",
        "        dev_acc, dev_loss, _ = eval_epoch(model, tokenizer, dev_dataset, batch_size, 'dev')\n",
        "        logger.info(f\"epoch: {epoch+1}, transformer: {name}, dev_loss: {dev_loss:.4f}, dev_acc: {dev_acc*100:.2f}\")\n",
        "\n",
        "        test_acc, test_loss, test_evaluation_metrics = eval_epoch(model, tokenizer, test_dataset,\n",
        "                                                                  batch_size, 'test')\n",
        "        logger.info(f\"epoch: {epoch+1}, transformer: {name}, test_loss: {test_loss:.4f}, test_acc: {test_acc*100:.2f}\")\n",
        "        logger.info(f\"epoch: {epoch+1}, transformer: {name}, \"\n",
        "                    f\"test_precision: {test_evaluation_metrics['test_precision']*100:.2f}, \"\n",
        "                    f\"test_recall: {test_evaluation_metrics['test_recall']*100:.2f}, \"\n",
        "                    f\"test_f1_score: {test_evaluation_metrics['test_f1_score']*100:.2f}, \"\n",
        "                    f\"test_accuracy_score: {test_evaluation_metrics['test_accuracy']*100:.2f}\")\n",
        "        logger.info(f\"epoch: {epoch+1}, transformer: {name}, test_confusion_matrix: \\n\"\n",
        "                    f\"{test_evaluation_metrics['test_confusion_matrix']}\")\n",
        "\n",
        "        logger.info(f\"Total training time elapsed: {timedelta(seconds=total_train_seconds)}\")\n",
        "        logger.info(f\"Mean time per train epoch: {timedelta(seconds=total_train_seconds/(epoch+1))}\")\n",
        "\n",
        "        #save best model and delete previous ones...\n",
        "        if save:\n",
        "            if test_acc > best_acc:\n",
        "                best_acc = test_acc\n",
        "                phrase_type, label = root_and_binary_title(root, binary)\n",
        "                dir_path = '/content/drive/My Drive/Model'\n",
        "                model_name = os.path.join(dir_path,\n",
        "                \"{}_{}_{}_{}.pickle\".format(name, phrase_type, label, epoch+1))\n",
        "                save_model(model, model_name, best_model_name)\n",
        "                best_model_name = model_name\n",
        "\n",
        "\n",
        "        # Implement early stopping here\n",
        "        if test_loss < best_loss:\n",
        "            best_loss = test_loss\n",
        "            stopping_step = 0\n",
        "        else:\n",
        "            stopping_step += 1\n",
        "\n",
        "        if stopping_step >= patience:\n",
        "            logger.info(\"EarlyStopping!\")\n",
        "            os._exit(1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "axYwav8wdiou",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9f51d787-ca1a-4144-bfe6-f1c3fddd437c"
      },
      "source": [
        "train('bert', True, False, 10, 3, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2020-09-04 07:16:20.962 | INFO     | dataset:__init__:17 - Preparing dataset config root: True, binary: False, split: train!\n",
            "2020-09-04 07:16:28.420 | INFO     | dataset:__init__:17 - Preparing dataset config root: True, binary: False, split: dev!\n",
            "2020-09-04 07:16:33.063 | INFO     | dataset:__init__:17 - Preparing dataset config root: True, binary: False, split: test!\n",
            "train: 100%|██████████| 267/267 [00:47<00:00,  5.59batch/s]\n",
            "2020-09-04 07:17:21.381 | INFO     | __main__:train:39 - epoch: 1, transformer: bert, train_loss: 0.0430, train_acc: 40.31\n",
            "dev: 100%|██████████| 35/35 [00:02<00:00, 17.08batch/s]\n",
            "2020-09-04 07:17:23.446 | INFO     | __main__:train:42 - epoch: 1, transformer: bert, dev_loss: 0.0381, dev_acc: 45.59\n",
            "test: 100%|██████████| 70/70 [00:04<00:00, 16.72batch/s]\n",
            "2020-09-04 07:17:27.650 | INFO     | __main__:train:46 - epoch: 1, transformer: bert, test_loss: 0.0366, test_acc: 49.55\n",
            "2020-09-04 07:17:27.650 | INFO     | __main__:train:47 - epoch: 1, transformer: bert, test_precision: 46.81, test_recall: 42.97, test_f1_score: 39.53, test_accuracy_score: 49.55\n",
            "2020-09-04 07:17:27.651 | INFO     | __main__:train:52 - epoch: 1, transformer: bert, test_confusion_matrix: \n",
            "[[ 16 232   8  23   0]\n",
            " [ 13 504  40  73   3]\n",
            " [  3 198  37 137  14]\n",
            " [  0  43  23 272 172]\n",
            " [  0   4  14 115 266]]\n",
            "2020-09-04 07:17:27.652 | INFO     | __main__:train:55 - Total training time elapsed: 0:00:47.787086\n",
            "2020-09-04 07:17:27.653 | INFO     | __main__:train:56 - Mean time per train epoch: 0:00:47.787086\n",
            "train: 100%|██████████| 267/267 [00:48<00:00,  5.56batch/s]\n",
            "2020-09-04 07:18:17.386 | INFO     | __main__:train:39 - epoch: 2, transformer: bert, train_loss: 0.0340, train_acc: 52.70\n",
            "dev: 100%|██████████| 35/35 [00:02<00:00, 17.33batch/s]\n",
            "2020-09-04 07:18:19.418 | INFO     | __main__:train:42 - epoch: 2, transformer: bert, dev_loss: 0.0358, dev_acc: 50.68\n",
            "test: 100%|██████████| 70/70 [00:04<00:00, 16.88batch/s]\n",
            "2020-09-04 07:18:23.582 | INFO     | __main__:train:46 - epoch: 2, transformer: bert, test_loss: 0.0337, test_acc: 53.26\n",
            "2020-09-04 07:18:23.583 | INFO     | __main__:train:47 - epoch: 2, transformer: bert, test_precision: 51.92, test_recall: 49.18, test_f1_score: 49.10, test_accuracy_score: 53.26\n",
            "2020-09-04 07:18:23.584 | INFO     | __main__:train:52 - epoch: 2, transformer: bert, test_confusion_matrix: \n",
            "[[103 152  15   9   0]\n",
            " [ 73 462  56  42   0]\n",
            " [ 14 202  65 101   7]\n",
            " [  1  41  38 326 104]\n",
            " [  1   9  15 153 221]]\n",
            "2020-09-04 07:18:23.585 | INFO     | __main__:train:55 - Total training time elapsed: 0:01:35.845391\n",
            "2020-09-04 07:18:23.585 | INFO     | __main__:train:56 - Mean time per train epoch: 0:00:47.922696\n",
            "train: 100%|██████████| 267/267 [00:47<00:00,  5.59batch/s]\n",
            "2020-09-04 07:19:13.024 | INFO     | __main__:train:39 - epoch: 3, transformer: bert, train_loss: 0.0291, train_acc: 60.59\n",
            "dev: 100%|██████████| 35/35 [00:02<00:00, 17.42batch/s]\n",
            "2020-09-04 07:19:15.047 | INFO     | __main__:train:42 - epoch: 3, transformer: bert, dev_loss: 0.0372, dev_acc: 51.41\n",
            "test: 100%|██████████| 70/70 [00:04<00:00, 16.77batch/s]\n",
            "2020-09-04 07:19:19.240 | INFO     | __main__:train:46 - epoch: 3, transformer: bert, test_loss: 0.0355, test_acc: 52.22\n",
            "2020-09-04 07:19:19.241 | INFO     | __main__:train:47 - epoch: 3, transformer: bert, test_precision: 51.82, test_recall: 51.09, test_f1_score: 51.11, test_accuracy_score: 52.22\n",
            "2020-09-04 07:19:19.241 | INFO     | __main__:train:52 - epoch: 3, transformer: bert, test_confusion_matrix: \n",
            "[[135 109  26   9   0]\n",
            " [123 333 122  54   1]\n",
            " [ 25 120 125 113   6]\n",
            " [  1  19  55 335 100]\n",
            " [  0   3  18 152 226]]\n",
            "2020-09-04 07:19:19.242 | INFO     | __main__:train:55 - Total training time elapsed: 0:02:23.637327\n",
            "2020-09-04 07:19:19.243 | INFO     | __main__:train:56 - Mean time per train epoch: 0:00:47.879109\n",
            "train: 100%|██████████| 267/267 [00:47<00:00,  5.59batch/s]\n",
            "2020-09-04 07:20:06.972 | INFO     | __main__:train:39 - epoch: 4, transformer: bert, train_loss: 0.0246, train_acc: 67.46\n",
            "dev: 100%|██████████| 35/35 [00:02<00:00, 17.01batch/s]\n",
            "2020-09-04 07:20:09.044 | INFO     | __main__:train:42 - epoch: 4, transformer: bert, dev_loss: 0.0393, dev_acc: 49.77\n",
            "test: 100%|██████████| 70/70 [00:04<00:00, 16.72batch/s]\n",
            "2020-09-04 07:20:13.249 | INFO     | __main__:train:46 - epoch: 4, transformer: bert, test_loss: 0.0373, test_acc: 53.26\n",
            "2020-09-04 07:20:13.249 | INFO     | __main__:train:47 - epoch: 4, transformer: bert, test_precision: 53.66, test_recall: 50.31, test_f1_score: 50.86, test_accuracy_score: 53.26\n",
            "2020-09-04 07:20:13.250 | INFO     | __main__:train:52 - epoch: 4, transformer: bert, test_confusion_matrix: \n",
            "[[ 86 148  32  13   0]\n",
            " [ 51 391 124  65   2]\n",
            " [  6 127 131 114  11]\n",
            " [  0  16  48 318 128]\n",
            " [  0   1  17 130 251]]\n",
            "2020-09-04 07:20:13.251 | INFO     | __main__:train:55 - Total training time elapsed: 0:03:11.364525\n",
            "2020-09-04 07:20:13.252 | INFO     | __main__:train:56 - Mean time per train epoch: 0:00:47.841131\n",
            "train: 100%|██████████| 267/267 [00:47<00:00,  5.61batch/s]\n",
            "2020-09-04 07:21:00.891 | INFO     | __main__:train:39 - epoch: 5, transformer: bert, train_loss: 0.0202, train_acc: 74.88\n",
            "dev: 100%|██████████| 35/35 [00:02<00:00, 17.17batch/s]\n",
            "2020-09-04 07:21:02.942 | INFO     | __main__:train:42 - epoch: 5, transformer: bert, dev_loss: 0.0417, dev_acc: 50.68\n",
            "test: 100%|██████████| 70/70 [00:04<00:00, 16.70batch/s]\n",
            "2020-09-04 07:21:07.152 | INFO     | __main__:train:46 - epoch: 5, transformer: bert, test_loss: 0.0400, test_acc: 52.26\n",
            "2020-09-04 07:21:07.153 | INFO     | __main__:train:47 - epoch: 5, transformer: bert, test_precision: 54.07, test_recall: 50.07, test_f1_score: 50.82, test_accuracy_score: 52.26\n",
            "2020-09-04 07:21:07.154 | INFO     | __main__:train:52 - epoch: 5, transformer: bert, test_confusion_matrix: \n",
            "[[109 121  38  11   0]\n",
            " [ 76 344 158  54   1]\n",
            " [ 10 117 152 107   3]\n",
            " [  0   9  65 366  70]\n",
            " [  0   4  18 193 184]]\n",
            "2020-09-04 07:21:07.154 | INFO     | __main__:train:55 - Total training time elapsed: 0:03:59.003531\n",
            "2020-09-04 07:21:07.155 | INFO     | __main__:train:56 - Mean time per train epoch: 0:00:47.800706\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xJ12BnAei0FS",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pXqhawGkdigP",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnoGNYJw6o6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc9L97qF_o8V",
        "colab_type": "text"
      },
      "source": [
        "# **BERT XGBoost**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAqqQAgf6o98",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4dcc081b-79f2-4052-8e46-29766ac70c3b"
      },
      "source": [
        "!pip install xgboost\n",
        "!pip install optuna\n",
        "!pip install pytreebank\n",
        "!pip install tqdm\n",
        "!pip install loguru\n",
        "!pip install transformers\n",
        "!pip install scikit-learn\n",
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (0.90)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.4.1)\n",
            "Collecting optuna\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/b0/9a6313c78bca92abfacc08a2ad8b27bfe845256f615786ee2b6452ae1978/optuna-2.0.0.tar.gz (226kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 3.3MB/s \n",
            "\u001b[?25hCollecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/1e/cabc75a189de0fbb2841d0975243e59bde8b7822bacbb95008ac6fe9ad47/alembic-1.4.2.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 25.6MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cliff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/06/03b1f92d46546a18eabf33ff7f37ef422c18c93d5a926bf590fee32ebe75/cliff-3.4.0-py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.3MB/s \n",
            "\u001b[?25hCollecting cmaes>=0.5.1\n",
            "  Downloading https://files.pythonhosted.org/packages/63/88/d5e9b78151dce671d7e78ee4cc8905d83208254caa2a386b163ae0ab0027/cmaes-0.6.0-py3-none-any.whl\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/2a/81/12d77537c82c5d46aa2721dfee25a0e873ef5920ebd0827152f411effb57/colorlog-4.2.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from optuna) (20.4)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.3.19)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna) (4.41.1)\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (2.8.1)\n",
            "Collecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: PrettyTable<0.8,>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (0.7.2)\n",
            "Collecting cmd2!=0.8.3,>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/20/ca2ddf5bb14c460d1b103a75ce51fe9e27c380091a3d532f9adc73922d73/cmd2-1.3.9-py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 30.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (3.13)\n",
            "Collecting stevedore>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/58/f80bdd6a86550e48d3766036ea77e5bc63a4b58bd581b8b8ceaa3111b901/stevedore-3.2.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.4.7)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/a3/d439f338aa90edd5ad9096cd56564b44882182150e92148eb14ceb7488ba/pbr-5.5.0-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 35.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna) (1.1.1)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (20.1.0)\n",
            "Requirement already satisfied: setuptools>=34.4 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (49.6.0)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/f6/5b/55866e1cde0f86f5eec59dab5de8a66628cb0d53da74b8dbc15ad8dabda3/pyperclip-1.8.0.tar.gz\n",
            "Requirement already satisfied: importlib-metadata>=1.6.0; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (1.7.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.2.5)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=1.6.0; python_version < \"3.8\"->cmd2!=0.8.3,>=0.8.0->cliff->optuna) (3.1.0)\n",
            "Building wheels for collected packages: alembic\n",
            "  Building wheel for alembic (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.2-cp36-none-any.whl size=159540 sha256=2d01d46df17239d0eb5d199ce857a1853dd114c2bf19f7d73b516352f7fb973f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/04/83/76023f7a4c14688c0b5c2682a96392cfdd3ee4449eaaa287ef\n",
            "Successfully built alembic\n",
            "Building wheels for collected packages: optuna, pyperclip\n",
            "  Building wheel for optuna (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-2.0.0-cp36-none-any.whl size=312967 sha256=228583f0eb226b518b7cd44fcc3e1c7d6d4ef030e49b820b86fd00f32ef244b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/c9/03/c45484454bf657ffed0ed6af153bd3d213928df115eb2a56eb\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.0-cp36-none-any.whl size=8693 sha256=eba676dc50becbed839b4c39ffb6f0d5e615b48eb7da1043a8136faf743c501f\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/ac/0a/b784f0afe26eaf52e88a7e15c7369090deea0354fa1c6fc689\n",
            "Successfully built optuna pyperclip\n",
            "Installing collected packages: Mako, python-editor, alembic, pyperclip, colorama, cmd2, pbr, stevedore, cliff, cmaes, colorlog, optuna\n",
            "Successfully installed Mako-1.1.3 alembic-1.4.2 cliff-3.4.0 cmaes-0.6.0 cmd2-1.3.9 colorama-0.4.3 colorlog-4.2.1 optuna-2.0.0 pbr-5.5.0 pyperclip-1.8.0 python-editor-1.0.4 stevedore-3.2.1\n",
            "Collecting pytreebank\n",
            "  Downloading https://files.pythonhosted.org/packages/e0/12/626ead6f6c0a0a9617396796b965961e9dfa5e78b36c17a81ea4c43554b1/pytreebank-0.2.7.tar.gz\n",
            "Building wheels for collected packages: pytreebank\n",
            "  Building wheel for pytreebank (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytreebank: filename=pytreebank-0.2.7-cp36-none-any.whl size=37070 sha256=13ee6bd41840a256c8c5db84a535f9ae39514e32b1a63e11b675d944b16833ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/b6/91/e9edcdbf464f623628d5c3aa9de28888c726e270b9a29f2368\n",
            "Successfully built pytreebank\n",
            "Installing collected packages: pytreebank\n",
            "Successfully installed pytreebank-0.2.7\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Collecting loguru\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/4f/baee593c195cd4b56cf008c9473347f3b0795b47d3b946e03706a8b43fca/loguru-0.5.1-py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.2MB/s \n",
            "\u001b[?25hCollecting aiocontextvars>=0.2.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/db/c1/7a723e8d988de0a2e623927396e54b6831b68cb80dce468c945b849a9385/aiocontextvars-0.2.2-py2.py3-none-any.whl\n",
            "Collecting contextvars==2.4; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz\n",
            "Collecting immutables>=0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/e0/ea6fd4697120327d26773b5a84853f897a68e33d3f9376b00a8ff96e4f63/immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 5.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: contextvars\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-cp36-none-any.whl size=7666 sha256=a64fe9cf314d67b7081dd8e598a7b912f42484bb38e9e358726b0db3db43b0e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de\n",
            "Successfully built contextvars\n",
            "Installing collected packages: immutables, contextvars, aiocontextvars, loguru\n",
            "Successfully installed aiocontextvars-0.2.2 contextvars-2.4 immutables-0.14 loguru-0.5.1\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 15.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 22.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 32.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=b7700fe2f35b32fde7d73d93e0b37cf06bf9bcf1966de5157a4dce50dd865ceb\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.1.0\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.16.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9bWtBs5_57Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from dataset import SSTDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import optuna\n",
        "import os\n",
        "from transformers import BertTokenizer\n",
        "import xgboost as xgb\n",
        "from utils import evaluation_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xycdC2mmA4us",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "daad6df6-8c0d-4e22-f7d9-97a99fc0c6fb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVLDIlB7AWFN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "47866f26-ef1e-4be0-a6ec-2a3ccb3705e2"
      },
      "source": [
        "model_path = os.path.join('/content/drive/My Drive/', 'Model/bert_root_fine_2.pickle')\n",
        "model_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Model/bert_root_fine_2.pickle'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQEvTU_97mo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = torch.load(model_path)\n",
        "#print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlFH6vaY_1u6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertFeatures(torch.nn.Module):\n",
        "  def __init__(self, model):\n",
        "    super(BertFeatures, self).__init__()\n",
        "    self.model = model.bert\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    return self.model(inputs['input_ids'], attention_mask=inputs['attention_mask'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRS-srx3BQkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_model = BertFeatures(model)\n",
        "#print(features_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyCepm024k36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root = True\n",
        "binary = False\n",
        "batch_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXjCiQYfekYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtjGUUlhBaKy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "14df40a2-f891-4978-d468-c225ffed3dd2"
      },
      "source": [
        "train_dataset = SSTDataset(root=root, binary=binary, split='train')\n",
        "dev_dataset = SSTDataset(root=root, binary=binary, split='dev')\n",
        "test_dataset = SSTDataset(root=root, binary=binary, split='test')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-05 05:28:43.261 | INFO     | dataset:__init__:17 - Preparing dataset config root: True, binary: False, split: train!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-09-05 05:28:55.012 | INFO     | dataset:__init__:17 - Preparing dataset config root: True, binary: False, split: dev!\n",
            "2020-09-05 05:29:00.933 | INFO     | dataset:__init__:17 - Preparing dataset config root: True, binary: False, split: test!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaRsUaiX3V8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def BERT_forward(text, model, tokenizer):\n",
        "  encoded_text = tokenizer(text, padding=True, return_tensors='pt').to(device)\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    last_hidden_state = model(encoded_text)[1]\n",
        "\n",
        "  return last_hidden_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rseZhMGybcyv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "71b7cbff29974f2c99df56bdda992d2a",
            "ffd8b17ddb3e4833a723cfd8d5cdfef1",
            "e909b455cf40422a8b9e2fcaa86c4281",
            "d3bb65ba840248858a4f4fbc0f814a42",
            "5da3171966e5435eb2776fe5ce80ae35",
            "7584ebfe41cf4062b5498ce661dccfd2",
            "d363ca9ad8f448fe9e457b9bd3ea533c",
            "d5919865e97442899d4e79435b667f0c"
          ]
        },
        "outputId": "95a5e035-b3c2-4888-de5a-5ca4da59942e"
      },
      "source": [
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "param = {'max_depth': 10, 'eta': 1, 'objective': 'multi:softmax'}\n",
        "param['nthread'] = 4\n",
        "param['eval_metric'] = 'mlogloss'\n",
        "param['num_class'] = 5\n",
        "\n",
        "num_round = 10\n",
        "\n",
        "y_actual = list()\n",
        "y_pred = list()\n",
        "mlog_loss = 0.0\n",
        "batch_no = 0\n",
        "\n",
        "for text, sentiment in train_loader:\n",
        "  y_actual += sentiment.numpy().tolist()\n",
        "  features = BERT_forward(text, features_model, tokenizer)\n",
        "  features = features.cpu().numpy()\n",
        "  #print(features.shape)\n",
        "  \n",
        "  dtrain = xgb.DMatrix(data=features, label=sentiment.numpy())\n",
        "  if batch_no == 0:\n",
        "    bst = xgb.train(param, dtrain, num_round)\n",
        "  else:\n",
        "    bst = xgb.train(param, dtrain, num_round, xgb_model=bst)\n",
        "        \n",
        "  mlog_loss += float(bst.eval(dtrain, name='mlogloss').split(':')[1])\n",
        "  print(\"loss: {}\".format(mlog_loss))\n",
        "  y_pred += bst.predict(data=dtrain).tolist()  \n",
        "  \n",
        "  print(\"iteration {} completed!\".format(batch_no))\n",
        "  batch_no += 1\n",
        "  \n",
        "print(\"Mean loss: {}\".format(mlog_loss/batch_no))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71b7cbff29974f2c99df56bdda992d2a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "loss: 0.075301\n",
            "iteration 0 completed!\n",
            "loss: 0.16329\n",
            "iteration 1 completed!\n",
            "loss: 0.323939\n",
            "iteration 2 completed!\n",
            "loss: 0.416531\n",
            "iteration 3 completed!\n",
            "loss: 0.516445\n",
            "iteration 4 completed!\n",
            "loss: 0.770362\n",
            "iteration 5 completed!\n",
            "loss: 0.875898\n",
            "iteration 6 completed!\n",
            "loss: 1.1988539999999999\n",
            "iteration 7 completed!\n",
            "loss: 1.772706\n",
            "iteration 8 completed!\n",
            "loss: 4.738552\n",
            "iteration 9 completed!\n",
            "loss: 5.985766\n",
            "iteration 10 completed!\n",
            "loss: 6.672102\n",
            "iteration 11 completed!\n",
            "loss: 8.00964\n",
            "iteration 12 completed!\n",
            "loss: 8.392202999999999\n",
            "iteration 13 completed!\n",
            "loss: 9.559890999999999\n",
            "iteration 14 completed!\n",
            "loss: 10.678220999999999\n",
            "iteration 15 completed!\n",
            "loss: 12.126195\n",
            "iteration 16 completed!\n",
            "loss: 14.447215\n",
            "iteration 17 completed!\n",
            "loss: 16.881285\n",
            "iteration 18 completed!\n",
            "loss: 19.761946\n",
            "iteration 19 completed!\n",
            "loss: 22.446106999999998\n",
            "iteration 20 completed!\n",
            "loss: 25.902942999999997\n",
            "iteration 21 completed!\n",
            "loss: 26.848027999999996\n",
            "iteration 22 completed!\n",
            "loss: 30.147035999999996\n",
            "iteration 23 completed!\n",
            "loss: 30.782766999999996\n",
            "iteration 24 completed!\n",
            "loss: 32.703329\n",
            "iteration 25 completed!\n",
            "loss: 34.64090899999999\n",
            "iteration 26 completed!\n",
            "loss: 36.34607299999999\n",
            "iteration 27 completed!\n",
            "loss: 36.81251999999999\n",
            "iteration 28 completed!\n",
            "loss: 37.73356799999999\n",
            "iteration 29 completed!\n",
            "loss: 41.07595899999999\n",
            "iteration 30 completed!\n",
            "loss: 44.65795699999999\n",
            "iteration 31 completed!\n",
            "loss: 48.79783699999999\n",
            "iteration 32 completed!\n",
            "loss: 54.321924999999986\n",
            "iteration 33 completed!\n",
            "loss: 55.67943299999999\n",
            "iteration 34 completed!\n",
            "loss: 63.389386999999985\n",
            "iteration 35 completed!\n",
            "loss: 65.50683299999999\n",
            "iteration 36 completed!\n",
            "loss: 69.23267399999999\n",
            "iteration 37 completed!\n",
            "loss: 71.26767099999999\n",
            "iteration 38 completed!\n",
            "loss: 75.615888\n",
            "iteration 39 completed!\n",
            "loss: 77.195838\n",
            "iteration 40 completed!\n",
            "loss: 81.751064\n",
            "iteration 41 completed!\n",
            "loss: 83.449197\n",
            "iteration 42 completed!\n",
            "loss: 87.465356\n",
            "iteration 43 completed!\n",
            "loss: 90.309013\n",
            "iteration 44 completed!\n",
            "loss: 91.95522399999999\n",
            "iteration 45 completed!\n",
            "loss: 94.354402\n",
            "iteration 46 completed!\n",
            "loss: 98.49117899999999\n",
            "iteration 47 completed!\n",
            "loss: 106.49457299999999\n",
            "iteration 48 completed!\n",
            "loss: 110.53714499999998\n",
            "iteration 49 completed!\n",
            "loss: 114.84590299999998\n",
            "iteration 50 completed!\n",
            "loss: 122.76986799999997\n",
            "iteration 51 completed!\n",
            "loss: 126.42287299999997\n",
            "iteration 52 completed!\n",
            "loss: 130.00971099999998\n",
            "iteration 53 completed!\n",
            "loss: 133.31794899999997\n",
            "iteration 54 completed!\n",
            "loss: 136.57370299999997\n",
            "iteration 55 completed!\n",
            "loss: 140.43943899999996\n",
            "iteration 56 completed!\n",
            "loss: 143.77370499999998\n",
            "iteration 57 completed!\n",
            "loss: 148.99673499999997\n",
            "iteration 58 completed!\n",
            "loss: 152.44728199999997\n",
            "iteration 59 completed!\n",
            "loss: 157.61890499999998\n",
            "iteration 60 completed!\n",
            "loss: 163.79635499999998\n",
            "iteration 61 completed!\n",
            "loss: 168.87733899999998\n",
            "iteration 62 completed!\n",
            "loss: 170.50104899999997\n",
            "iteration 63 completed!\n",
            "loss: 177.79691399999996\n",
            "iteration 64 completed!\n",
            "loss: 182.46012599999995\n",
            "iteration 65 completed!\n",
            "loss: 184.80928199999994\n",
            "iteration 66 completed!\n",
            "loss: 187.45865699999993\n",
            "iteration 67 completed!\n",
            "loss: 193.32477599999993\n",
            "iteration 68 completed!\n",
            "loss: 195.40739099999993\n",
            "iteration 69 completed!\n",
            "loss: 198.51811499999994\n",
            "iteration 70 completed!\n",
            "loss: 203.03039399999994\n",
            "iteration 71 completed!\n",
            "loss: 207.57574599999995\n",
            "iteration 72 completed!\n",
            "loss: 209.97665099999995\n",
            "iteration 73 completed!\n",
            "loss: 214.81674199999995\n",
            "iteration 74 completed!\n",
            "loss: 216.91607099999996\n",
            "iteration 75 completed!\n",
            "loss: 221.33891999999997\n",
            "iteration 76 completed!\n",
            "loss: 227.04237299999997\n",
            "iteration 77 completed!\n",
            "loss: 229.33223699999996\n",
            "iteration 78 completed!\n",
            "loss: 232.51064299999996\n",
            "iteration 79 completed!\n",
            "loss: 236.15895999999995\n",
            "iteration 80 completed!\n",
            "loss: 238.91022799999996\n",
            "iteration 81 completed!\n",
            "loss: 244.17324699999995\n",
            "iteration 82 completed!\n",
            "loss: 249.13499699999994\n",
            "iteration 83 completed!\n",
            "loss: 256.1097239999999\n",
            "iteration 84 completed!\n",
            "loss: 261.8389259999999\n",
            "iteration 85 completed!\n",
            "loss: 264.9400279999999\n",
            "iteration 86 completed!\n",
            "loss: 271.9608349999999\n",
            "iteration 87 completed!\n",
            "loss: 273.7376929999999\n",
            "iteration 88 completed!\n",
            "loss: 279.02206799999993\n",
            "iteration 89 completed!\n",
            "loss: 281.80337899999995\n",
            "iteration 90 completed!\n",
            "loss: 288.05148199999996\n",
            "iteration 91 completed!\n",
            "loss: 293.85092499999996\n",
            "iteration 92 completed!\n",
            "loss: 300.76819299999994\n",
            "iteration 93 completed!\n",
            "loss: 305.41522599999996\n",
            "iteration 94 completed!\n",
            "loss: 309.84796099999994\n",
            "iteration 95 completed!\n",
            "loss: 313.10548299999994\n",
            "iteration 96 completed!\n",
            "loss: 316.0447009999999\n",
            "iteration 97 completed!\n",
            "loss: 321.64170799999994\n",
            "iteration 98 completed!\n",
            "loss: 326.5087689999999\n",
            "iteration 99 completed!\n",
            "loss: 334.8064799999999\n",
            "iteration 100 completed!\n",
            "loss: 336.9768299999999\n",
            "iteration 101 completed!\n",
            "loss: 340.7705489999999\n",
            "iteration 102 completed!\n",
            "loss: 345.1294609999999\n",
            "iteration 103 completed!\n",
            "loss: 348.1064919999999\n",
            "iteration 104 completed!\n",
            "loss: 352.8995619999999\n",
            "iteration 105 completed!\n",
            "loss: 358.82759099999987\n",
            "iteration 106 completed!\n",
            "loss: 363.2223519999999\n",
            "iteration 107 completed!\n",
            "loss: 369.6006849999999\n",
            "iteration 108 completed!\n",
            "loss: 373.74588699999987\n",
            "iteration 109 completed!\n",
            "loss: 378.74120399999987\n",
            "iteration 110 completed!\n",
            "loss: 384.5419889999999\n",
            "iteration 111 completed!\n",
            "loss: 388.3995239999999\n",
            "iteration 112 completed!\n",
            "loss: 394.92855199999985\n",
            "iteration 113 completed!\n",
            "loss: 397.83183299999985\n",
            "iteration 114 completed!\n",
            "loss: 405.76177399999983\n",
            "iteration 115 completed!\n",
            "loss: 412.25477099999983\n",
            "iteration 116 completed!\n",
            "loss: 418.3321639999998\n",
            "iteration 117 completed!\n",
            "loss: 423.1099049999998\n",
            "iteration 118 completed!\n",
            "loss: 429.0564609999998\n",
            "iteration 119 completed!\n",
            "loss: 433.3625239999998\n",
            "iteration 120 completed!\n",
            "loss: 438.58024699999976\n",
            "iteration 121 completed!\n",
            "loss: 442.23226099999977\n",
            "iteration 122 completed!\n",
            "loss: 448.5502889999998\n",
            "iteration 123 completed!\n",
            "loss: 454.9492909999998\n",
            "iteration 124 completed!\n",
            "loss: 460.37675999999976\n",
            "iteration 125 completed!\n",
            "loss: 464.48019199999976\n",
            "iteration 126 completed!\n",
            "loss: 467.7938799999998\n",
            "iteration 127 completed!\n",
            "loss: 475.53551199999976\n",
            "iteration 128 completed!\n",
            "loss: 481.5176289999998\n",
            "iteration 129 completed!\n",
            "loss: 483.0370549999998\n",
            "iteration 130 completed!\n",
            "loss: 487.8027199999998\n",
            "iteration 131 completed!\n",
            "loss: 492.0816849999998\n",
            "iteration 132 completed!\n",
            "loss: 498.16187899999983\n",
            "iteration 133 completed!\n",
            "loss: 502.3480599999998\n",
            "iteration 134 completed!\n",
            "loss: 508.4495759999998\n",
            "iteration 135 completed!\n",
            "loss: 515.9833109999998\n",
            "iteration 136 completed!\n",
            "loss: 520.3267439999998\n",
            "iteration 137 completed!\n",
            "loss: 525.5751379999998\n",
            "iteration 138 completed!\n",
            "loss: 530.6337299999998\n",
            "iteration 139 completed!\n",
            "loss: 536.6748639999998\n",
            "iteration 140 completed!\n",
            "loss: 541.9325129999999\n",
            "iteration 141 completed!\n",
            "loss: 548.4955299999998\n",
            "iteration 142 completed!\n",
            "loss: 553.8928969999998\n",
            "iteration 143 completed!\n",
            "loss: 560.0374029999998\n",
            "iteration 144 completed!\n",
            "loss: 565.4560839999998\n",
            "iteration 145 completed!\n",
            "loss: 571.1563999999998\n",
            "iteration 146 completed!\n",
            "loss: 576.1032149999999\n",
            "iteration 147 completed!\n",
            "loss: 578.5763619999999\n",
            "iteration 148 completed!\n",
            "loss: 582.3896619999999\n",
            "iteration 149 completed!\n",
            "loss: 588.073358\n",
            "iteration 150 completed!\n",
            "loss: 591.844611\n",
            "iteration 151 completed!\n",
            "loss: 598.134919\n",
            "iteration 152 completed!\n",
            "loss: 603.4420259999999\n",
            "iteration 153 completed!\n",
            "loss: 606.899827\n",
            "iteration 154 completed!\n",
            "loss: 609.3828669999999\n",
            "iteration 155 completed!\n",
            "loss: 615.7015089999999\n",
            "iteration 156 completed!\n",
            "loss: 620.6986409999998\n",
            "iteration 157 completed!\n",
            "loss: 625.0949639999999\n",
            "iteration 158 completed!\n",
            "loss: 626.6490309999999\n",
            "iteration 159 completed!\n",
            "loss: 632.7816049999999\n",
            "iteration 160 completed!\n",
            "loss: 636.9807399999999\n",
            "iteration 161 completed!\n",
            "loss: 643.0831869999998\n",
            "iteration 162 completed!\n",
            "loss: 647.8613899999998\n",
            "iteration 163 completed!\n",
            "loss: 651.7076739999998\n",
            "iteration 164 completed!\n",
            "loss: 655.0661139999997\n",
            "iteration 165 completed!\n",
            "loss: 661.1069769999997\n",
            "iteration 166 completed!\n",
            "loss: 664.6308199999997\n",
            "iteration 167 completed!\n",
            "loss: 669.9232529999997\n",
            "iteration 168 completed!\n",
            "loss: 673.8651349999997\n",
            "iteration 169 completed!\n",
            "loss: 680.6382159999997\n",
            "iteration 170 completed!\n",
            "loss: 685.3418199999998\n",
            "iteration 171 completed!\n",
            "loss: 688.4718849999997\n",
            "iteration 172 completed!\n",
            "loss: 695.8426059999997\n",
            "iteration 173 completed!\n",
            "loss: 699.9971069999997\n",
            "iteration 174 completed!\n",
            "loss: 707.5900409999997\n",
            "iteration 175 completed!\n",
            "loss: 713.5069689999997\n",
            "iteration 176 completed!\n",
            "loss: 718.4649759999996\n",
            "iteration 177 completed!\n",
            "loss: 726.9181629999996\n",
            "iteration 178 completed!\n",
            "loss: 732.4538499999996\n",
            "iteration 179 completed!\n",
            "loss: 736.9368279999997\n",
            "iteration 180 completed!\n",
            "loss: 740.3995859999997\n",
            "iteration 181 completed!\n",
            "loss: 744.7289349999996\n",
            "iteration 182 completed!\n",
            "loss: 752.0611259999996\n",
            "iteration 183 completed!\n",
            "loss: 757.3973899999996\n",
            "iteration 184 completed!\n",
            "loss: 760.0772629999997\n",
            "iteration 185 completed!\n",
            "loss: 766.2782629999997\n",
            "iteration 186 completed!\n",
            "loss: 769.7777259999997\n",
            "iteration 187 completed!\n",
            "loss: 775.7788499999997\n",
            "iteration 188 completed!\n",
            "loss: 780.0189999999997\n",
            "iteration 189 completed!\n",
            "loss: 786.1531929999996\n",
            "iteration 190 completed!\n",
            "loss: 791.7429879999996\n",
            "iteration 191 completed!\n",
            "loss: 799.8641349999996\n",
            "iteration 192 completed!\n",
            "loss: 804.6650749999995\n",
            "iteration 193 completed!\n",
            "loss: 809.3817939999996\n",
            "iteration 194 completed!\n",
            "loss: 815.9172919999995\n",
            "iteration 195 completed!\n",
            "loss: 823.7715439999995\n",
            "iteration 196 completed!\n",
            "loss: 829.9766709999994\n",
            "iteration 197 completed!\n",
            "loss: 834.9192899999995\n",
            "iteration 198 completed!\n",
            "loss: 839.1521329999995\n",
            "iteration 199 completed!\n",
            "loss: 845.0495729999994\n",
            "iteration 200 completed!\n",
            "loss: 849.0478649999994\n",
            "iteration 201 completed!\n",
            "loss: 856.1304669999994\n",
            "iteration 202 completed!\n",
            "loss: 860.5172429999994\n",
            "iteration 203 completed!\n",
            "loss: 863.6020389999994\n",
            "iteration 204 completed!\n",
            "loss: 866.0584709999994\n",
            "iteration 205 completed!\n",
            "loss: 873.1449599999994\n",
            "iteration 206 completed!\n",
            "loss: 878.5164819999994\n",
            "iteration 207 completed!\n",
            "loss: 883.8291059999995\n",
            "iteration 208 completed!\n",
            "loss: 887.9490799999994\n",
            "iteration 209 completed!\n",
            "loss: 895.8414139999994\n",
            "iteration 210 completed!\n",
            "loss: 900.6581809999994\n",
            "iteration 211 completed!\n",
            "loss: 906.6853779999994\n",
            "iteration 212 completed!\n",
            "loss: 910.7583839999994\n",
            "iteration 213 completed!\n",
            "loss: 916.9434549999994\n",
            "iteration 214 completed!\n",
            "loss: 921.0780519999994\n",
            "iteration 215 completed!\n",
            "loss: 923.3498779999994\n",
            "iteration 216 completed!\n",
            "loss: 930.9254149999995\n",
            "iteration 217 completed!\n",
            "loss: 934.9941919999994\n",
            "iteration 218 completed!\n",
            "loss: 940.3591419999995\n",
            "iteration 219 completed!\n",
            "loss: 943.5230639999994\n",
            "iteration 220 completed!\n",
            "loss: 947.9524099999994\n",
            "iteration 221 completed!\n",
            "loss: 953.4735219999994\n",
            "iteration 222 completed!\n",
            "loss: 956.7663579999994\n",
            "iteration 223 completed!\n",
            "loss: 963.3543659999993\n",
            "iteration 224 completed!\n",
            "loss: 967.0269629999993\n",
            "iteration 225 completed!\n",
            "loss: 969.7170579999994\n",
            "iteration 226 completed!\n",
            "loss: 973.4558319999994\n",
            "iteration 227 completed!\n",
            "loss: 977.2363819999994\n",
            "iteration 228 completed!\n",
            "loss: 978.6715509999993\n",
            "iteration 229 completed!\n",
            "loss: 983.8569059999993\n",
            "iteration 230 completed!\n",
            "loss: 989.9145879999993\n",
            "iteration 231 completed!\n",
            "loss: 995.8428359999994\n",
            "iteration 232 completed!\n",
            "loss: 1000.7681699999994\n",
            "iteration 233 completed!\n",
            "loss: 1006.2405419999993\n",
            "iteration 234 completed!\n",
            "loss: 1009.5171109999993\n",
            "iteration 235 completed!\n",
            "loss: 1015.5407569999993\n",
            "iteration 236 completed!\n",
            "loss: 1020.5874619999993\n",
            "iteration 237 completed!\n",
            "loss: 1023.7754829999993\n",
            "iteration 238 completed!\n",
            "loss: 1028.0128029999994\n",
            "iteration 239 completed!\n",
            "loss: 1034.5933979999993\n",
            "iteration 240 completed!\n",
            "loss: 1040.1164939999992\n",
            "iteration 241 completed!\n",
            "loss: 1044.4497629999992\n",
            "iteration 242 completed!\n",
            "loss: 1052.0060939999992\n",
            "iteration 243 completed!\n",
            "loss: 1058.1291449999992\n",
            "iteration 244 completed!\n",
            "loss: 1061.3367599999992\n",
            "iteration 245 completed!\n",
            "loss: 1068.5392919999993\n",
            "iteration 246 completed!\n",
            "loss: 1071.7577879999992\n",
            "iteration 247 completed!\n",
            "loss: 1076.7264199999993\n",
            "iteration 248 completed!\n",
            "loss: 1079.8441649999993\n",
            "iteration 249 completed!\n",
            "loss: 1084.6631889999992\n",
            "iteration 250 completed!\n",
            "loss: 1094.7335619999992\n",
            "iteration 251 completed!\n",
            "loss: 1100.1751809999992\n",
            "iteration 252 completed!\n",
            "loss: 1106.4383439999992\n",
            "iteration 253 completed!\n",
            "loss: 1112.2849479999993\n",
            "iteration 254 completed!\n",
            "loss: 1119.3683049999993\n",
            "iteration 255 completed!\n",
            "loss: 1123.3759949999992\n",
            "iteration 256 completed!\n",
            "loss: 1132.304213999999\n",
            "iteration 257 completed!\n",
            "loss: 1138.198411999999\n",
            "iteration 258 completed!\n",
            "loss: 1142.602748999999\n",
            "iteration 259 completed!\n",
            "loss: 1149.539687999999\n",
            "iteration 260 completed!\n",
            "loss: 1155.718398999999\n",
            "iteration 261 completed!\n",
            "loss: 1157.853243999999\n",
            "iteration 262 completed!\n",
            "loss: 1164.131364999999\n",
            "iteration 263 completed!\n",
            "loss: 1168.588297999999\n",
            "iteration 264 completed!\n",
            "loss: 1171.987964999999\n",
            "iteration 265 completed!\n",
            "loss: 1175.1164069999988\n",
            "iteration 266 completed!\n",
            "Mean loss: 4.401185044943816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Kn1pUix2WKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trial_BERT_XGBoost(trial):\n",
        "  train_loader = DataLoader(dataset=train_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True)\n",
        "  dev_loader = DataLoader(dataset=dev_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True)\n",
        "\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "  param = {'objective': 'multi:softmax',\n",
        "           \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
        "           \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
        "           \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),}\n",
        "\n",
        "\n",
        "  if param[\"booster\"] == \"gbtree\" or param[\"booster\"] == \"dart\":\n",
        "    param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n",
        "    param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
        "    param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
        "    param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
        "  if param[\"booster\"] == \"dart\":\n",
        "    param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
        "    param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
        "    param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
        "    param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
        "\n",
        "  param['eval_metric'] = 'mlogloss'\n",
        "  param['num_class'] = 5\n",
        "\n",
        "  num_round = 10\n",
        "\n",
        "  y_true = list()\n",
        "  y_pred = list()\n",
        "  mlog_loss = 0.0\n",
        "  batch_no = 0\n",
        "\n",
        "  for text, sentiment in train_loader:\n",
        "    y_true += sentiment.numpy().tolist()\n",
        "    features = BERT_forward(text, features_model, tokenizer)\n",
        "    features = features.cpu().numpy()\n",
        "    #print(features.shape)\n",
        "\n",
        "    dtrain = xgb.DMatrix(data=features, label=sentiment.numpy())\n",
        "    if batch_no == 0:\n",
        "      bst = xgb.train(param, dtrain)\n",
        "    else:\n",
        "      bst = xgb.train(param, dtrain, xgb_model=bst)\n",
        "\n",
        "    mlog_loss += float(bst.eval(dtrain, name='mlogloss').split(':')[1])\n",
        "    #print(\"loss: {}\".format(mlog_loss))\n",
        "    y_pred += bst.predict(data=dtrain).tolist()  \n",
        "\n",
        "    #print(\"iteration {} completed!\".format(batch_no))\n",
        "    batch_no += 1\n",
        "\n",
        "  #print(\"Mean loss: {}\".format(mlog_loss/batch_no))\n",
        "\n",
        "  y_true = list()\n",
        "  y_pred = list()\n",
        "  mlog_loss = 0.0\n",
        "  batch_no = 0\n",
        "\n",
        "  for text, sentiment in dev_loader:\n",
        "    y_true += sentiment.numpy().tolist()\n",
        "    features = BERT_forward(text, features_model, tokenizer)\n",
        "    features = features.cpu().numpy()\n",
        "    #print(features.shape)\n",
        "\n",
        "    ddev = xgb.DMatrix(data=features, label=sentiment.numpy())\n",
        "\n",
        "    mlog_loss += float(bst.eval(ddev, name='mlogloss').split(':')[1])\n",
        "    #print(\"loss: {}\".format(mlog_loss))\n",
        "    y_pred += bst.predict(data=ddev).tolist()  \n",
        "\n",
        "    #print(\"iteration {} completed!\".format(batch_no))\n",
        "    batch_no += 1\n",
        "\n",
        "  #print(\"Mean loss: {}\".format(mlog_loss/batch_no))\n",
        "  metrics = evaluation_metrics(y_true, y_pred, split='dev')\n",
        "\n",
        "  return metrics['dev_f1_score']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZt9TkKkCgJW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "09c4a720-f0be-49ed-c811-2624a5ad7045"
      },
      "source": [
        "study = optuna.create_study(direction='maximize')\n",
        "#study.optimize(trial_BERT_XGBoost, timeout=900)\n",
        "study.optimize(trial_BERT_XGBoost, n_trials=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-05 06:30:32,901] Trial 0 finished with value: 0.48785366745721515 and parameters: {'booster': 'gbtree', 'lambda': 1.6148183427140605e-05, 'alpha': 0.00026767384302040187, 'max_depth': 8, 'eta': 0.001019615693312664, 'gamma': 1.6859726228584748e-05, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.48785366745721515.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n",
            "[I 2020-09-05 06:30:55,744] Trial 1 finished with value: 0.08316546762589928 and parameters: {'booster': 'gblinear', 'lambda': 2.052367336165611e-08, 'alpha': 0.3742160379699689}. Best is trial 0 with value: 0.48785366745721515.\n",
            "[I 2020-09-05 06:31:18,786] Trial 2 finished with value: 0.05213270142180095 and parameters: {'booster': 'gblinear', 'lambda': 1.3537270162134945e-05, 'alpha': 0.04611024602630847}. Best is trial 0 with value: 0.48785366745721515.\n",
            "[I 2020-09-05 06:31:41,789] Trial 3 finished with value: 0.08316546762589928 and parameters: {'booster': 'gblinear', 'lambda': 4.565872894321703e-08, 'alpha': 2.141054354124851e-06}. Best is trial 0 with value: 0.48785366745721515.\n",
            "[I 2020-09-05 06:32:04,564] Trial 4 finished with value: 0.05213270142180095 and parameters: {'booster': 'gblinear', 'lambda': 0.029707708935356226, 'alpha': 1.2111427665820707e-05}. Best is trial 0 with value: 0.48785366745721515.\n",
            "[I 2020-09-05 06:33:02,196] Trial 5 finished with value: 0.4739908304428749 and parameters: {'booster': 'dart', 'lambda': 4.740057004948028e-05, 'alpha': 1.58615329787922e-08, 'max_depth': 5, 'eta': 5.106054081251027e-08, 'gamma': 0.1289419591893452, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.5630350229007507, 'skip_drop': 8.131488245619524e-05}. Best is trial 0 with value: 0.48785366745721515.\n",
            "[I 2020-09-05 06:34:08,926] Trial 6 finished with value: 0.451879119165932 and parameters: {'booster': 'dart', 'lambda': 0.0023588230867761023, 'alpha': 1.0528241652143961e-08, 'max_depth': 9, 'eta': 0.008967363685795655, 'gamma': 1.2069849678920367e-08, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.004028399396582589, 'skip_drop': 5.291964296013543e-06}. Best is trial 0 with value: 0.48785366745721515.\n",
            "[I 2020-09-05 06:34:56,045] Trial 7 finished with value: 0.4778005749100217 and parameters: {'booster': 'gbtree', 'lambda': 1.6141749105567e-06, 'alpha': 0.00897420090241327, 'max_depth': 9, 'eta': 5.036310286109422e-07, 'gamma': 2.67537452109486e-07, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.48785366745721515.\n",
            "[I 2020-09-05 06:35:53,539] Trial 8 finished with value: 0.47513742483691396 and parameters: {'booster': 'dart', 'lambda': 0.19340378911763753, 'alpha': 0.0013430583295655025, 'max_depth': 6, 'eta': 3.447738271019294e-05, 'gamma': 1.4836110634772616e-05, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.0013809548966749463, 'skip_drop': 0.00019021134352763746}. Best is trial 0 with value: 0.48785366745721515.\n",
            "[I 2020-09-05 06:36:16,460] Trial 9 finished with value: 0.05213270142180095 and parameters: {'booster': 'gblinear', 'lambda': 1.0952727422506721e-06, 'alpha': 8.0584291308346e-05}. Best is trial 0 with value: 0.48785366745721515.\n",
            "[I 2020-09-05 06:36:48,415] Trial 10 finished with value: 0.0688721804511278 and parameters: {'booster': 'gbtree', 'lambda': 0.001675330551229666, 'alpha': 1.0731633160152784e-06, 'max_depth': 1, 'eta': 0.477333885452362, 'gamma': 0.008552623444853205, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.48785366745721515.\n",
            "[I 2020-09-05 06:37:36,270] Trial 11 finished with value: 0.48525781025764425 and parameters: {'booster': 'gbtree', 'lambda': 6.640039172092782e-07, 'alpha': 0.002957232907731193, 'max_depth': 9, 'eta': 1.6456605625018124e-06, 'gamma': 1.2954404767144848e-06, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.48785366745721515.\n",
            "[I 2020-09-05 06:38:25,138] Trial 12 finished with value: 0.48103129042712317 and parameters: {'booster': 'gbtree', 'lambda': 5.367844980760761e-07, 'alpha': 0.0008398663310777627, 'max_depth': 8, 'eta': 5.610779391017251e-05, 'gamma': 8.855930328381517e-06, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.48785366745721515.\n",
            "[I 2020-09-05 06:39:24,322] Trial 13 finished with value: 0.49179939868028233 and parameters: {'booster': 'gbtree', 'lambda': 0.00022594500693159955, 'alpha': 0.00027903618094064184, 'max_depth': 7, 'eta': 0.0036597895590911225, 'gamma': 0.00047909973048988617, 'grow_policy': 'lossguide'}. Best is trial 13 with value: 0.49179939868028233.\n",
            "[I 2020-09-05 06:40:16,329] Trial 14 finished with value: 0.4639348583907438 and parameters: {'booster': 'gbtree', 'lambda': 0.0005743804812883386, 'alpha': 0.00014122406871253397, 'max_depth': 7, 'eta': 0.013223577204468023, 'gamma': 0.0009615541893574071, 'grow_policy': 'lossguide'}. Best is trial 13 with value: 0.49179939868028233.\n",
            "[I 2020-09-05 06:41:10,194] Trial 15 finished with value: 0.4960925407247142 and parameters: {'booster': 'gbtree', 'lambda': 2.2210101412847032e-05, 'alpha': 5.215254840673735e-05, 'max_depth': 3, 'eta': 0.0038505846921398834, 'gamma': 0.00014764249637132256, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:41:43,415] Trial 16 finished with value: 0.0808695652173913 and parameters: {'booster': 'gbtree', 'lambda': 0.00021134044659362674, 'alpha': 9.968769407285073e-06, 'max_depth': 2, 'eta': 0.20942523385102496, 'gamma': 0.002082582737431942, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:42:36,549] Trial 17 finished with value: 0.48901896903068015 and parameters: {'booster': 'gbtree', 'lambda': 0.025835288153299874, 'alpha': 1.9165601898758072e-07, 'max_depth': 3, 'eta': 0.0015350089064273158, 'gamma': 0.00024748054951617, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:43:20,273] Trial 18 finished with value: 0.426594423268019 and parameters: {'booster': 'gbtree', 'lambda': 7.873893829936141e-06, 'alpha': 4.080826618879809e-05, 'max_depth': 4, 'eta': 0.02769777148667243, 'gamma': 0.08731882963880469, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:44:13,465] Trial 19 finished with value: 0.4870886470837991 and parameters: {'booster': 'gbtree', 'lambda': 0.00011152041821358476, 'alpha': 0.013721035744832263, 'max_depth': 5, 'eta': 0.0007561857272684174, 'gamma': 0.008302230449445062, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:44:51,593] Trial 20 finished with value: 0.43117545435570037 and parameters: {'booster': 'gbtree', 'lambda': 0.00852803000214586, 'alpha': 0.29967058266924124, 'max_depth': 3, 'eta': 0.06480600322886787, 'gamma': 8.12991428028852e-05, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:45:39,930] Trial 21 finished with value: 0.4858593885765524 and parameters: {'booster': 'gbtree', 'lambda': 0.6934643928745522, 'alpha': 1.7169156221871857e-07, 'max_depth': 3, 'eta': 0.0013078242122451136, 'gamma': 0.00030720951597614594, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:46:16,464] Trial 22 finished with value: 0.48380973374369274 and parameters: {'booster': 'gbtree', 'lambda': 0.02655337633250821, 'alpha': 1.392963277652718e-07, 'max_depth': 1, 'eta': 0.0003026531285234937, 'gamma': 0.00013548516982784666, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:47:09,753] Trial 23 finished with value: 0.4857448060806381 and parameters: {'booster': 'gbtree', 'lambda': 0.0005567587336334175, 'alpha': 2.3379004616679832e-07, 'max_depth': 3, 'eta': 0.0041204069226119895, 'gamma': 0.0019934969702950812, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:47:56,533] Trial 24 finished with value: 0.4703528314971542 and parameters: {'booster': 'gbtree', 'lambda': 0.15725750741136355, 'alpha': 2.817589697172354e-06, 'max_depth': 4, 'eta': 1.2969103022140466e-05, 'gamma': 6.73103140303678e-05, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:48:42,431] Trial 25 finished with value: 0.4820483285793101 and parameters: {'booster': 'gbtree', 'lambda': 0.005574295694126693, 'alpha': 1.8566122854365957e-05, 'max_depth': 2, 'eta': 0.0001931282163298222, 'gamma': 0.010084087182869347, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:49:54,944] Trial 26 finished with value: 0.4897998950264735 and parameters: {'booster': 'dart', 'lambda': 3.9420880903546736e-06, 'alpha': 0.00044452761272987076, 'max_depth': 4, 'eta': 0.0035811386626382097, 'gamma': 2.3085891409687234e-06, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.8243203753719584e-08, 'skip_drop': 0.5553327847536946}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:50:42,220] Trial 27 finished with value: 0.4062418060721516 and parameters: {'booster': 'dart', 'lambda': 1.5464185326995981e-07, 'alpha': 0.000414464391448873, 'max_depth': 6, 'eta': 0.08674488316633909, 'gamma': 2.6030062718526004e-06, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.0438659661943758e-08, 'skip_drop': 0.12712058086288427}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:51:50,417] Trial 28 finished with value: 0.4709497579689372 and parameters: {'booster': 'dart', 'lambda': 6.705457106804294e-06, 'alpha': 0.004070555235914314, 'max_depth': 4, 'eta': 0.006434407743753933, 'gamma': 1.711354678069259e-07, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 3.7440968696990054e-08, 'skip_drop': 0.8559556155311191}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:52:30,088] Trial 29 finished with value: 0.0688721804511278 and parameters: {'booster': 'dart', 'lambda': 4.083248671074801e-05, 'alpha': 0.0002948190831063261, 'max_depth': 6, 'eta': 0.5983846760289373, 'gamma': 1.6936307270287587e-08, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 8.936965678272522e-06, 'skip_drop': 0.005103622634279032}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:53:19,600] Trial 30 finished with value: 0.4360602540947731 and parameters: {'booster': 'dart', 'lambda': 5.530476586479337e-06, 'alpha': 0.06836230393003469, 'max_depth': 7, 'eta': 0.05652182005603382, 'gamma': 4.410164418380308e-07, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 3.0008298280753283e-06, 'skip_drop': 1.9149306551372656e-08}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:54:05,563] Trial 31 finished with value: 0.49142659533415534 and parameters: {'booster': 'gbtree', 'lambda': 3.300403120978085e-05, 'alpha': 5.9423741921875664e-05, 'max_depth': 2, 'eta': 0.003177661578678722, 'gamma': 0.0004872758092968573, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:55:04,192] Trial 32 finished with value: 0.49033074407625643 and parameters: {'booster': 'dart', 'lambda': 3.114071852068648e-05, 'alpha': 4.927066883952553e-05, 'max_depth': 2, 'eta': 0.0036253441643362572, 'gamma': 0.0006302656558884029, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 1.9890325929291075e-07, 'skip_drop': 2.4882049330312776e-08}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:55:50,447] Trial 33 finished with value: 0.483511650129099 and parameters: {'booster': 'gbtree', 'lambda': 7.25794502296752e-05, 'alpha': 7.844219003687047e-05, 'max_depth': 2, 'eta': 0.0002674944099683816, 'gamma': 0.0011739018588916913, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:56:25,780] Trial 34 finished with value: 0.439368111976283 and parameters: {'booster': 'gbtree', 'lambda': 1.849546520079107e-05, 'alpha': 3.315879124813507e-05, 'max_depth': 1, 'eta': 0.015370672551093015, 'gamma': 0.0005398985702320049, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:56:48,652] Trial 35 finished with value: 0.0688721804511278 and parameters: {'booster': 'gblinear', 'lambda': 0.00018333882325983412, 'alpha': 4.445247452016247e-06}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:57:49,394] Trial 36 finished with value: 0.48572433582220065 and parameters: {'booster': 'dart', 'lambda': 2.0990881102075843e-05, 'alpha': 0.0001717798015317522, 'max_depth': 2, 'eta': 0.0007918658112659783, 'gamma': 4.156539822154655e-05, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 7.620059767304128e-07, 'skip_drop': 7.172952393494882e-08}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:58:12,294] Trial 37 finished with value: 0.0688721804511278 and parameters: {'booster': 'gblinear', 'lambda': 0.0004367572604125286, 'alpha': 9.756331353360124e-06}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:58:56,555] Trial 38 finished with value: 0.49183716184913423 and parameters: {'booster': 'gbtree', 'lambda': 3.6114516597463964e-05, 'alpha': 3.873507557728487e-05, 'max_depth': 2, 'eta': 0.0011912548741310652, 'gamma': 0.8621167410264295, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 06:59:32,967] Trial 39 finished with value: 0.4745340400965589 and parameters: {'booster': 'gbtree', 'lambda': 1.9048473980683727e-07, 'alpha': 1.1480855209647287e-06, 'max_depth': 1, 'eta': 9.873129326020216e-06, 'gamma': 0.3092246244819402, 'grow_policy': 'depthwise'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 07:00:26,039] Trial 40 finished with value: 0.4903147177411283 and parameters: {'booster': 'gbtree', 'lambda': 2.0780514371745485e-06, 'alpha': 0.0007417316311248277, 'max_depth': 3, 'eta': 0.0013837155978707574, 'gamma': 0.03309631256259276, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 07:01:09,925] Trial 41 finished with value: 0.48484183896568245 and parameters: {'booster': 'gbtree', 'lambda': 2.4100171960847105e-05, 'alpha': 7.675675212600847e-05, 'max_depth': 2, 'eta': 0.003320232360713212, 'gamma': 0.9205938285303978, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 07:01:57,182] Trial 42 finished with value: 0.49312758394423994 and parameters: {'booster': 'gbtree', 'lambda': 6.6663300934963e-05, 'alpha': 3.019311498239559e-05, 'max_depth': 2, 'eta': 0.0003866950568381122, 'gamma': 2.929041734063253e-05, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 07:02:42,510] Trial 43 finished with value: 0.48380374231000706 and parameters: {'booster': 'gbtree', 'lambda': 0.0015592669106414667, 'alpha': 2.5563500061865308e-05, 'max_depth': 2, 'eta': 0.00011205591110755264, 'gamma': 1.3357309821008505e-05, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 07:03:18,969] Trial 44 finished with value: 0.48830629257956437 and parameters: {'booster': 'gbtree', 'lambda': 7.914413872282025e-05, 'alpha': 6.907271428540712e-06, 'max_depth': 1, 'eta': 0.000691839938025434, 'gamma': 3.1652754901102364e-05, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 07:04:05,124] Trial 45 finished with value: 0.44802818999160676 and parameters: {'booster': 'gbtree', 'lambda': 0.0002534495753688867, 'alpha': 0.0001642999955638864, 'max_depth': 8, 'eta': 0.024857526755410333, 'gamma': 5.10977017932766e-06, 'grow_policy': 'depthwise'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 07:05:00,274] Trial 46 finished with value: 0.48229546745748875 and parameters: {'booster': 'gbtree', 'lambda': 1.3022599765107931e-05, 'alpha': 0.0017293917705651295, 'max_depth': 5, 'eta': 0.00038791204467856453, 'gamma': 0.00017260840181929378, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 07:05:47,200] Trial 47 finished with value: 0.4741495312018243 and parameters: {'booster': 'gbtree', 'lambda': 0.00014809568834032655, 'alpha': 1.661007642183113e-05, 'max_depth': 3, 'eta': 1.0854699424595986e-08, 'gamma': 0.0032988131395242895, 'grow_policy': 'lossguide'}. Best is trial 15 with value: 0.4960925407247142.\n",
            "[I 2020-09-05 07:06:39,256] Trial 48 finished with value: 0.481560992592254 and parameters: {'booster': 'gbtree', 'lambda': 4.763172842926851e-05, 'alpha': 1.2768653801923608e-06, 'max_depth': 7, 'eta': 3.319266760662533e-05, 'gamma': 2.603403353195294e-05, 'grow_policy': 'depthwise'}. Best is trial 15 with value: 0.4960925407247142.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-ecc74b34d473>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#study.optimize(trial_BERT_XGBoost, timeout=900)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_BERT_XGBoost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 self._optimize_sequential(\n\u001b[0;32m--> 292\u001b[0;31m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m                 )\n\u001b[1;32m    294\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks, gc_after_trial, time_start)\u001b[0m\n\u001b[1;32m    652\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_progress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;31m# type: (...) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch, gc_after_trial)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Trial {} pruned. {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-6a14eafcc745>\u001b[0m in \u001b[0;36mtrial_BERT_XGBoost\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERT_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;31m#print(features.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJm7lKMQBbCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T55FOokYCh3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiZTK0m8Ch80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olwmyvNxCh65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}